{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk = pd.read_csv('train.csv', chunksize = 500000, low_memory = False)\n",
    "chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid = [0]*500000\n",
    "coord_list = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'fare_amount']\n",
    "def mark_invalid(chunk):\n",
    "    for c in coord_list:\n",
    "        for i in chunk.index:\n",
    "            if(c == \"pickup_longitude\" or c == \"dropoff_longitude\"):\n",
    "                if(chunk[c][i].astype(float) > -73.699215 or chunk[c][i].astype(float) < -74.257159):\n",
    "#                     chunk = chunk.replace(chunk[c][i],np.nan)\n",
    "#                     chunk = chunk.drop([i])\n",
    "                    invalid[i%500000] = 1\n",
    "            elif (c == \"pickup_latitude\" or c == \"dropoff_latitude\"):\n",
    "                if(chunk[c][i].astype(float) > 40.915568 or chunk[c][i].astype(float) < 40.495992):\n",
    "#                     chunk = chunk.replace(chunk[c][i],np.nan)\n",
    "#                     chunk = chunk.drop([i])\n",
    "                    invalid[i%500000] = 1\n",
    "            elif(c == \"fare_amount\"):\n",
    "                if(chunk[c][i] >= 200 or chunk[c][i] <= 0):\n",
    "#                     chunk = chunk.drop([i])\n",
    "                    invalid[i%500000] = 1\n",
    "    return chunk\n",
    "\n",
    "invalid_test = [0]*1000000\n",
    "coord_list_test = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "def mark_invalid_test(chunk):\n",
    "    for c in coord_list_test:\n",
    "        for i in chunk.index:\n",
    "            if(c == \"pickup_longitude\" or c == \"dropoff_longitude\"):\n",
    "                if(chunk[c][i].astype(float) > -73.699215 or chunk[c][i].astype(float) < -74.257159):\n",
    "#                     chunk = chunk.replace(chunk[c][i],np.nan)\n",
    "#                     chunk = chunk.drop([i])\n",
    "                    invalid_test[i%1000000] = 1\n",
    "            elif (c == \"pickup_latitude\" or c == \"dropoff_latitude\"):\n",
    "                if(chunk[c][i].astype(float) > 40.915568 or chunk[c][i].astype(float) < 40.495992):\n",
    "#                     chunk = chunk.replace(chunk[c][i],np.nan)\n",
    "#                     chunk = chunk.drop([i])\n",
    "                    invalid_test[i%1000000] = 1\n",
    "    return chunk\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_outlier(chunk, data_1):\n",
    "    outliers_indices=[]\n",
    "    threshold = 3\n",
    "    mean_1 = np.mean(data_1)\n",
    "    std_1 = np.std(data_1)\n",
    "    \n",
    "#     length = len(data_1)\n",
    "    for i in chunk.index:\n",
    "        z_score= (data_1[i] - mean_1)/std_1 \n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers_indices.append(i)\n",
    "    for i in outliers_indices:\n",
    "#         chunk = chunk.drop([i])\n",
    "        chunk['invalid'][i] = 1\n",
    "    return chunk\n",
    "\n",
    "\n",
    "# print(len(detect_outlier(chunks[1]['fare_amount'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datetime(chunk):\n",
    "    hours = []\n",
    "    mins = []\n",
    "    secs = []\n",
    "    years = []\n",
    "    months = []\n",
    "    days = []\n",
    "    length = len(chunk['pickup_longitude'])\n",
    "    \n",
    "    for i in chunk.index:\n",
    "        years.append(int(chunk['pickup_datetime'][i][0:4]))\n",
    "        months.append(int(chunk['pickup_datetime'][i][5:7]) - 1) # 1 is subtracted to aid in days from jan 1st calculations\n",
    "        days.append(int(chunk['pickup_datetime'][i][8:10]))\n",
    "        hours.append(int(chunk['pickup_datetime'][i][11:13]))\n",
    "        mins.append(int(chunk['pickup_datetime'][i][14:16]))\n",
    "        secs.append(int(chunk['pickup_datetime'][i][17:19]))\n",
    "\n",
    "    chunk['years'] = years\n",
    "    chunk['months'] = months\n",
    "    chunk['days'] = days\n",
    "    chunk['hours'] = hours\n",
    "    chunk['mins'] = mins\n",
    "    chunk['secs'] = secs\n",
    "    \n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_datetime(chunk):\n",
    "    chunk['secs_past_midnight'] = (chunk['hours']*3600) + (chunk['mins']*60) + (chunk['secs'])\n",
    "    chunk['sin_spm'] = np.sin(2*np.pi*(chunk['secs_past_midnight']/86400))\n",
    "    chunk['cos_spm'] = np.cos(2*np.pi*(chunk['secs_past_midnight']/86400))\n",
    "    chunk['days_past_jan1'] = (chunk['months']*30) + (chunk['days'])\n",
    "    chunk['sin_dpj'] = np.sin(2*np.pi*(chunk['days_past_jan1']/365))\n",
    "    chunk['cos_dpj'] = np.cos(2*np.pi*(chunk['days_past_jan1']/365))\n",
    "    \n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(chunk):\n",
    "    y = chunk['fare_amount']\n",
    "    X = pd.DataFrame(chunk)\n",
    "    X = X.drop(['fare_amount','key','pickup_datetime', 'years', 'months', 'days', 'hours', 'mins', 'secs', 'secs_past_midnight', 'days_past_jan1'], axis = 1)\n",
    "#     X = StandardScaler().fit_transform(X)\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_rmse(X, y, linreg = LinearRegression()):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "#     linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_pred = linreg.predict(X_test)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    return (linreg, rmse)\n",
    "\n",
    "# X1_train, X1_test, y1_train, y1_test = train_test_split(features, target, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename): \n",
    "#     filename = 'model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename): \n",
    "    model = pickle.load(open(filename, 'rb'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_columns_test(chunk):\n",
    "    temp = pd.DataFrame(chunk)\n",
    "    temp = temp.drop(['key','pickup_datetime', 'years', 'months', 'days', 'hours', 'mins', 'secs', 'secs_past_midnight', 'days_past_jan1'], axis = 1)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading chunks and applying functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.338667972014402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.689230615870558\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for chunk in df_chunk:\n",
    "    chunk = pd.DataFrame(chunk)\n",
    "    if (count == 0):\n",
    "        model = LinearRegression()\n",
    "    else:\n",
    "        model = load_model(\"model.sav\")\n",
    "        \n",
    "    chunk = mark_invalid(chunk)\n",
    "    chunk['invalid'] = invalid\n",
    "    chunk.dropna(inplace = True)\n",
    "    chunk = split_datetime(chunk)\n",
    "    chunk = modify_datetime(chunk)\n",
    "    chunk = mark_outlier(chunk, chunk['fare_amount'])\n",
    "    chunk['manhattan_dist'] = abs(chunk['pickup_latitude']-chunk['dropoff_latitude']) + abs(chunk['pickup_longitude']-chunk['dropoff_longitude'])\n",
    "    (X1, y1) = split_data(chunk)\n",
    "    X1 = pd.DataFrame(X1)\n",
    "    model, rmse = fit_model_rmse(X1, y1, model)\n",
    "    save_model(model, \"model.sav\")\n",
    "    print(rmse)\n",
    "    \n",
    "    if (count == 0):\n",
    "        chunks.append(pd.DataFrame(chunk))\n",
    "    count += 1\n",
    "    if(count == 2):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "test_chunk = pd.read_csv('test.csv', chunksize = 1000000, low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 21)\n",
      "(1000000, 11)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ad8ac433cbbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mfinal_dfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    330\u001b[0m                     \u001b[0;34m\" only Series and DataFrame objs are valid\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 )\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(pd.DataFrame(test_chunk).shape)\n",
    "test_chunks = []\n",
    "final_dfs = []\n",
    "for chunk in test_chunk:\n",
    "    chunk = pd.DataFrame(chunk)\n",
    "    if(count == 0):\n",
    "        model = load_model(\"model.sav\")\n",
    "    chunk = mark_invalid_test(chunk)\n",
    "    chunk['invalid'] = invalid_test\n",
    "    chunk = split_datetime(chunk)\n",
    "    chunk = modify_datetime(chunk)\n",
    "    chunk['manhattan_dist'] = abs(chunk['pickup_latitude']-chunk['dropoff_latitude']) + abs(chunk['pickup_longitude']-chunk['dropoff_longitude'])\n",
    "    if(count == 0):\n",
    "        test_chunks.append(pd.DataFrame(chunk))\n",
    "    print(chunk.shape)\n",
    "    req_df = req_columns_test(chunk)\n",
    "    print(req_df.shape)\n",
    "    y_pred = model.predict(req_df)\n",
    "    df = pd.concat([chunk['key'], y_pred], axis = 1)\n",
    "    final_dfs.append(df)\n",
    "    \n",
    "    count += 1\n",
    "    break\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunks[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_chunks[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_chunks[0].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "print(chunks[0][\"pickup_longitude\"].plot.hist(ax = ax, title=\"pickup longitude\",bottom=1, bins=25))\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "print(chunks[0][\"pickup_latitude\"].plot.hist(ax = ax, title=\"pickup latitude\",bottom=1, bins=25))\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "print(chunks[0][\"dropoff_longitude\"].plot.hist(ax = ax, title=\"dropoff longitude\",bottom=1, bins=25))\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "print(chunks[0][\"dropoff_latitude\"].plot.hist(ax = ax, title=\"dropoff latitude\",bottom=1, bins=25))\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "print(chunks[0][\"passenger_count\"].plot.hist(ax = ax, title=\"passenger count\",bottom=1, bins=25))\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex = True, sharey = True)\n",
    "ax1.scatter(chunks[0][\"pickup_latitude\"],chunks[0][\"fare_amount\"])\n",
    "# ax1.xlabel(\"pickup_latitude\")\n",
    "# ax1.ylabel(\"fare_amount\")\n",
    "ax2.scatter(chunks[0][\"pickup_longitude\"],chunks[0][\"fare_amount\"])\n",
    "# ax2.xlabel(\"pickup_longitude\")\n",
    "# ax2.ylabel(\"fare_amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(chunks[0][\"dropoff_latitude\"],chunks[0][\"fare_amount\"])\n",
    "plt.xlabel(\"dropoff_latitude\")\n",
    "plt.ylabel(\"fare_amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(chunks[0][\"dropoff_longitude\"],chunks[0][\"fare_amount\"])\n",
    "plt.xlabel(\"dropoff_longitude\")\n",
    "plt.ylabel(\"fare_amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(chunks[0][\"passenger_count\"],chunks[0][\"fare_amount\"])\n",
    "plt.xlabel(\"passenger_count\")\n",
    "plt.ylabel(\"fare_amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(chunks[0].fare_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0].to_csv('preprocessed.csv')\n",
    "chunk1 = pd.read_csv('preprocessed.csv')\n",
    "chunks.append(chunk1)\n",
    "chunks[1].head()\n",
    "chunks[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = detect_outlier(chunks[1]['fare_amount'])\n",
    "# for i in indices:\n",
    "#     chunks[1] = chunks[1].drop([i])\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[1].to_csv('preprocessed1.csv')\n",
    "chunks.append(pd.read_csv('preprocessed1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricting coordinates to NYC\n",
    "\n",
    "All coordinates outside NYC are directly dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing all rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(chunks[0])\n",
    "# print(chunks[0].isnull().sum())\n",
    "# chunks[0].dropna(inplace = True)\n",
    "# print(chunks[0].isnull().sum())\n",
    "# chunks[0]['pickup_datetime'][0]\n",
    "# chunks[0].describe()\n",
    "# type(chunks[0]['dropoff_longitude'][161652])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting pickup date time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating secs after midnight and days past jan 1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks[0] = chunks[0].drop(['Unnamed: 0', 'Unnamed: 0.1'], axis = 1)\n",
    "chunks[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks[0]['manhattan_dist'] = abs(chunks[0]['pickup_latitude']-chunks[0]['dropoff_latitude']) + abs(chunks[0]['pickup_longitude']-chunks[0]['dropoff_longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = chunks[0].drop(['fare_amount','key','pickup_datetime', 'years', 'months', 'days', 'hours', 'mins', 'secs', 'secs_past_midnight', 'days_past_jan1'], axis = 1)\n",
    "# target = chunks[0]['fare_amount']\n",
    "# features = StandardScaler().fit_transform(features)\n",
    "\n",
    "# pca = PCA(n_components=6)\n",
    "# principalComponents = pca.fit_transform(features)\n",
    "# principalDf = pd.DataFrame(data = principalComponents, columns = ['PrincipalC1', 'PrincipalC2', 'PrincipalC3','PrincipalC4', 'PrincipalC5', 'PrincipalC6'])\n",
    "# finalDf = pd.concat([principalDf, target], axis=1)\n",
    "# finalDf.head()\n",
    "# chunks[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test train split & Linear Regression & RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# print(X1_train.shape)\n",
    "# print(y1_train.shape)\n",
    "# print(X1_test.shape)\n",
    "# print(y1_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linreg1 = LinearRegression()\n",
    "# linreg1.fit(X1_train, y1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linreg.predict(X_test)\n",
    "# y1_pred = linreg1.predict(X1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without PCA: \", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "# print(\"With PCA: \", np.sqrt(metrics.mean_squared_error(y1_test, y1_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
